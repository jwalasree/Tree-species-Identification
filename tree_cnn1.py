# -*- coding: utf-8 -*-
"""tree_CNN1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rln8FBDOYBmifkCnBJFASMV7jyXldptw
"""

!git clone https://github.com/jwalasree/Tree-species-Identification.git

import os

repo_path = "/content/Tree-species-Identification"  # Adjust if cloned to a different name
print("Contents:", os.listdir(repo_path))

class_dirs = os.listdir(repo_path)
print(f"Number of classes: {len(class_dirs)}")

import glob

image_paths = []
labels = []

for class_name in os.listdir(repo_path):
    class_folder = os.path.join(repo_path, class_name)
    if os.path.isdir(class_folder):
        for img_file in os.listdir(class_folder):
            image_paths.append(os.path.join(class_folder, img_file))
            labels.append(class_name)

print(f"Total images: {len(image_paths)}")

import os
import pandas as pd

# Replace with your actual image dataset path
repo_path = r"/content/Tree-species-Identification"

# Get class folders
class_dirs = [d for d in os.listdir(repo_path) if os.path.isdir(os.path.join(repo_path, d))]

# Count files in each folder
class_counts = {
    cls: len([
        f for f in os.listdir(os.path.join(repo_path, cls))
        if os.path.isfile(os.path.join(repo_path, cls, f))
    ])
    for cls in class_dirs
}

# Build DataFrame
class_counts_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Image Count'])

# Output
print(class_counts_df.sort_values('Image Count', ascending=False).head())
print("Shape:", class_counts_df.shape)

import pandas as pd

class_counts = {cls: len(os.listdir(os.path.join(repo_path, cls))) for cls in class_dirs}
class_counts_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Image Count'])
print(class_counts_df.sort_values('Image Count', ascending=False).head())
print("shape: ",class_counts_df.shape)

import os
import matplotlib.pyplot as plt
from PIL import Image

def show_sample_images(repo_path, class_dirs, n=5):
    plt.figure(figsize=(15, 10))
    for i, class_dir in enumerate(class_dirs[:n]):
        folder_path = os.path.join(repo_path, class_dir)

        # Filter only image files
        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

        if not files:
            print(f"No image files found in {folder_path}")
            continue

        img_path = os.path.join(folder_path, files[0])
        img = Image.open(img_path)

        plt.subplot(1, n, i+1)
        plt.imshow(img)
        plt.title(class_dir)
        plt.axis('off')

    plt.show()

import os
from PIL import Image
import pandas as pd

image_shapes = []

# Loop through each class directory
for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)

    # Filter only image files (by extension)
    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    for img_file in image_files:
        img_path = os.path.join(class_path, img_file)
        try:
            img = Image.open(img_path)
            image_shapes.append(img.size)
        except Exception as e:
            print(f"Skipping file {img_path} due to error: {e}")

# Create DataFrame
shapes_df = pd.DataFrame(image_shapes, columns=["Width", "Height"])
shapes_df.head()

print(shapes_df.describe())

import os
import hashlib
from collections import defaultdict

hashes = defaultdict(list)

# Allowed image extensions
valid_exts = ('.png', '.jpg', '.jpeg')

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)

    for img_file in os.listdir(class_path):
        if not img_file.lower().endswith(valid_exts):
            continue  # Skip non-image files

        img_path = os.path.join(class_path, img_file)
        try:
            with open(img_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
                hashes[file_hash].append(img_path)
        except Exception as e:
            print(f"Error reading {img_path}: {e}")

# Filter duplicates
duplicates = {h: files for h, files in hashes.items() if len(files) > 1}

print("Duplicate image sets found:", len(duplicates))

# Optional: Print first few duplicate sets
for i, (h, files) in enumerate(duplicates.items()):
    print(f"\nDuplicate set {i+1}:")
    for f in files:
        print(f)
    if i >= 2:  # Show only first 3 sets
        break

corrupt_images = []

for class_dir in class_dirs:
    for img_file in os.listdir(os.path.join(repo_path, class_dir)):
        img_path = os.path.join(repo_path, class_dir, img_file)
        try:
            img = Image.open(img_path)
            img.verify()
        except Exception as e:
            corrupt_images.append(img_path)

print("Corrupted images:", len(corrupt_images))

import matplotlib.pyplot as plt
from PIL import Image

def show_duplicate_sets(duplicates, sets_to_display=3):
    shown = 0
    for hash_val, dup_paths in duplicates.items():
        if shown >= sets_to_display:
            break
        print(f"Duplicate set {shown + 1}:")
        plt.figure(figsize=(15, 4))
        for i, img_path in enumerate(dup_paths):
            try:
                img = Image.open(img_path)
                plt.subplot(1, len(dup_paths), i+1)
                plt.imshow(img)
                plt.title(os.path.basename(img_path))
                plt.axis('off')
            except:
                continue
        plt.show()
        shown += 1

show_duplicate_sets(duplicates, sets_to_display=3)

import os

removed_count = 0

for dup_list in duplicates.values():
    # Keep the first, delete rest
    for img_path in dup_list[1:]:
        try:
            os.remove(img_path)
            removed_count += 1
        except Exception as e:
            print(f"Error deleting {img_path}: {e}")

print(f"‚úÖ Removed {removed_count} duplicate images.")

import os
import hashlib
from collections import defaultdict

hashes = defaultdict(list)
valid_exts = ('.jpg', '.jpeg', '.png')  # Add more if needed

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)

    for img_file in os.listdir(class_path):
        if not img_file.lower().endswith(valid_exts):
            continue  # Skip non-image files

        img_path = os.path.join(class_path, img_file)
        try:
            with open(img_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
                hashes[file_hash].append(img_path)
        except Exception as e:
            print(f"Error reading {img_path}: {e}")

# Filter only hashes with more than 1 image (duplicates)
duplicates = {h: files for h, files in hashes.items() if len(files) > 1}

print("üîÅ Duplicates remaining:", len(duplicates))

import os
from PIL import Image
import pandas as pd

image_info = []
valid_exts = ('.png', '.jpg', '.jpeg')  # Filter image files

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)
    for img_file in os.listdir(class_path):
        if not img_file.lower().endswith(valid_exts):
            continue  # Skip non-images

        img_path = os.path.join(class_path, img_file)
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                image_info.append({
                    'path': img_path,
                    'width': width,
                    'height': height,
                    'class': class_dir
                })
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {img_path} due to error: {e}")
            continue

# Now convert to DataFrame
df_sizes = pd.DataFrame(image_info)

# ‚úÖ Confirm columns exist
print("Columns:", df_sizes.columns)

# Define thresholds
if not df_sizes.empty and 'width' in df_sizes.columns:
    small_imgs = df_sizes[(df_sizes['width'] < 150) | (df_sizes['height'] < 150)]
    large_imgs = df_sizes[(df_sizes['width'] > 1000) | (df_sizes['height'] > 2000)]

    print(f"üîª Very small images: {len(small_imgs)}")
    print(f"üî∫ Very large images: {len(large_imgs)}")
else:
    print("‚ùå No valid image data found.")

import os
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt

# Setup
valid_exts = ('.png', '.jpg', '.jpeg')
image_info = []

# Step 1: Collect image info
for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)
    for img_file in os.listdir(class_path):
        if not img_file.lower().endswith(valid_exts):
            continue  # Skip non-images

        img_path = os.path.join(class_path, img_file)
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                image_info.append({
                    'path': img_path,
                    'width': width,
                    'height': height,
                    'class': class_dir
                })
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {img_path} due to error: {e}")
            continue

# Step 2: Create DataFrame
df_sizes = pd.DataFrame(image_info)

# Step 3: Filter small & large images
small_imgs = df_sizes[(df_sizes['width'] < 150) | (df_sizes['height'] < 150)] if not df_sizes.empty else pd.DataFrame()
large_imgs = df_sizes[(df_sizes['width'] > 1000) | (df_sizes['height'] > 2000)] if not df_sizes.empty else pd.DataFrame()

print(f"üîª Very small images: {len(small_imgs)}")
print(f"üî∫ Very large images: {len(large_imgs)}")

# Step 4: Define function to show image samples
def show_images(df_subset, title, n=5):
    if df_subset.empty:
        print(f"‚ö†Ô∏è No images to show for: {title}")
        return

    plt.figure(figsize=(15, 3))
    for i, (_, row) in enumerate(df_subset.head(n).iterrows()):
        try:
            img = Image.open(row['path'])
            plt.subplot(1, n, i+1)
            plt.imshow(img)
            plt.title(f"{row['width']}x{row['height']}")
            plt.axis('off')
        except Exception as e:
            print(f"‚ùå Couldn't open image {row['path']}: {e}")
            continue
    plt.suptitle(title)
    plt.show()

# Step 5: Show small & large images
show_images(small_imgs, "Very Small Images")
show_images(large_imgs, "Very Large Images")

import os
import pandas as pd

# Combine small and large outliers
outliers = pd.concat([small_imgs, large_imgs])

# Debug: Check columns
print("‚úÖ Columns in outliers DataFrame:", outliers.columns.tolist())

# Remove only if 'path' column exists
if 'path' in outliers.columns:
    removed_count = 0

    for path in outliers['path']:
        try:
            os.remove(path)
            removed_count += 1
        except Exception as e:
            print(f"‚ùå Failed to remove {path}: {e}")

    print(f"üóëÔ∏è Successfully removed {removed_count} outlier images.")
else:
    print("‚ö†Ô∏è Column 'path' not found in outliers DataFrame. No files were removed.")

import os
from PIL import Image
import pandas as pd

# Recalculate sizes with filtering
image_info = []
valid_exts = ('.png', '.jpg', '.jpeg')

for class_dir in class_dirs:
    class_path = os.path.join(repo_path, class_dir)
    for img_file in os.listdir(class_path):
        if not img_file.lower().endswith(valid_exts):
            continue  # Skip non-images

        img_path = os.path.join(class_path, img_file)
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                image_info.append({
                    'path': img_path,
                    'width': width,
                    'height': height,
                    'class': class_dir
                })
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping {img_path}: {e}")
            continue

# Create DataFrame
df_sizes = pd.DataFrame(image_info)

# Debug print
print("üìä Columns in DataFrame:", df_sizes.columns.tolist())
print("üßæ First few rows:")
print(df_sizes.head())

# Only show stats if width & height are present
if {'width', 'height'}.issubset(df_sizes.columns):
    print(df_sizes[['width', 'height']].describe())
else:
    print("‚ùå 'width' and 'height' columns not found in df_sizes.")

!pip install tensorflow==2.18.0 numpy==1.26.4

import os
os.kill(os.getpid(), 9)  # This restarts the Colab runtime

!ls Tree-species-Identification

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Updated path
repo_path = "Tree-species-Identification/Tree_Species_Dataset"

# Check path
if not os.path.exists(repo_path):
    raise FileNotFoundError(f"‚ùå Dataset not found at: {repo_path}")

# Parameters
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Data generator
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    horizontal_flip=True
)

# Training and validation generators
train_gen = datagen.flow_from_directory(
    repo_path,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_gen = datagen.flow_from_directory(
    repo_path,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam

base_model = EfficientNetB0(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), weights='imagenet')
base_model.trainable = False  # Freeze base model

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(train_gen.num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

EPOCHS = 10

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS
)

# Plot accuracy/loss curves
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy Over Epochs")
plt.show()

# Save model
model.save("tree_species_model.h5")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define image properties
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Data generators
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

train_generator = datagen.flow_from_directory(
    repo_path,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    repo_path,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# Number of output classes
num_classes = train_generator.num_classes

# Build a basic CNN model
model_cnn = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model_cnn.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Train the model
EPOCHS = 10
history_cnn = model_cnn.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator
)

# Save the model
model_cnn.save('basic_cnn_tree_species.h5')
print("‚úÖ Basic CNN model saved as 'basic_cnn_tree_species.h5'")

import matplotlib.pyplot as plt

plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')
plt.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')
plt.title("Basic CNN Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(30, activation='softmax')  # For 30 classes
])

model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(train_generator, validation_data=val_generator, epochs=25)

model.save("improved_cnn_model.h5")